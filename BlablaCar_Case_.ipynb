{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO96cR5zaKJGibCOPj+N+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayRayKing/blablacar/blob/main/BlablaCar_Case_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from airflow.decorators import dag, task\n",
        "\n",
        "# Retry logic with exponential backoff\n",
        "@retry(\n",
        "    stop=stop_after_attempt(5),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=60),  # Exponential backoff: 2, 4, 8, 16, 32 seconds\n",
        "    retry=retry_if_exception_type(requests.exceptions.RequestException)  # Retry on HTTP request errors\n",
        ")\n",
        "@task\n",
        "def get_data() -> str :\n",
        "    \"\"\"returns filename that has \"\"\"\n",
        "\n",
        "\n",
        "    base_url = \"http://v0.ovapi.nl/\"\n",
        "    endpoint = \"line/\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url + endpoint)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "        data = response.json()\n",
        "        ## Convert to Ndjson for easier load to BQ.\n",
        "        # Also allows handling of scd within down stream transformation instead of touching the EL process.\n",
        "        # less schema enforcement with this method.\n",
        "\n",
        "\n",
        "        output_file = 'processed_json_data.njson'\n",
        "        with open(output_file, 'w') as f:\n",
        "            for line_id, line_info in data.items():\n",
        "                # Write each JSON object on a new line\n",
        "                json.dump({line_id: line_info}, f)\n",
        "                f.write('\\n')  # Add newline after each JSON object\n",
        "        print(f\"NDJSON data saved to {output_file}\")\n",
        "\n",
        "        ## In real world, file would be saved to cloud (GSC bucket and returns gsi)\n",
        "        return output_file\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data from the API: {e}\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QUMRpPkY6vQK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##DML for BQ\n",
        "CREATE TABLE IF NOT EXISTS dataset_name.nl_lines_data_raw (\n",
        "    id STRING,\n",
        "    line_data JSON\n",
        ") PARTITION BY DATE(_PARTITIONTIME);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyDahXo47uCR",
        "outputId": "1d563378-d82b-49c1-85a5-8c6c00881a67"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data from the API: 504 Server Error: Gateway Time-out for url: http://v0.ovapi.nl/line/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DDL For inserting to BQ\n",
        "\n",
        "import json\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def insert_to_bq(gsi_filepath, dataset_id, table_id):\n",
        "    \"\"\"\n",
        "    Function to load data into BigQuery.\n",
        "    \"\"\"\n",
        "    # Initialize BigQuery client\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    # Define the dataset and table reference\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    # Define job configuration for loading JSON data\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "        autodetect=True,  # Automatically detect schema\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # Append to table\n",
        "    )\n",
        "\n",
        "    # Create a job to load the data\n",
        "    job = client.load_table_from_uri(\n",
        "        gsi_filepath,\n",
        "        table_ref,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    # Wait for the job to complete\n",
        "    job.result()\n",
        "\n",
        "\n",
        "# IDOMPOTENCY/ ACID\n",
        "# In this particular case, the goal is to delete partions and replace them for large sets of data. This method is a bit more cost saving.\n",
        "# an alternative is to use upserts/merge statemnents to handle the idompotency, but for larger data sets, it requires full table/partition scans.\n",
        "def delete_reloading_data(start_date,end_date,dataset_table,additional_filter=None):\n",
        "  \"\"\" Deletes data from the same loading timeframe (Date-based setup) \"\"\"\n",
        "  client = bigquery.Client()\n",
        "  query = (\n",
        "      f\"\"\"\n",
        "      DELETE FROM {dataset_table}\n",
        "      WHERE _PARTITIONTIME BETWEEN '{start_date}' AND '{end_date}' {additional_filter}\n",
        "      \"\"\"\n",
        "  )\n",
        "  client.query(query)\n"
      ],
      "metadata": {
        "id": "Ok9en7T_G0mS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.providers.python_operator import PythonOperator\n",
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\n",
        "from airflow.utils.dates import days_ago\n",
        "import os\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': days_ago(1),\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    'bq_load',\n",
        "    default_args=default_args,\n",
        "    description='loading data into bq',\n",
        "    schedule_interval=None #chron time setup\n",
        ") as dag:\n",
        "\n",
        "\n",
        "  # Task 1: get data & convert to ndjson & upload file to GCS\n",
        "  # Task creation via task api\n",
        "\n",
        "  # Task 2: create idempotency by deleting any possible similar timeframe data\n",
        "  delete_reloading_task = PythonOperator(\n",
        "      task_id='delete_reloading_data',\n",
        "      python_callable=delete_reloading_data,\n",
        "      op_kwargs={\n",
        "          'start_date': 'YYYY-MM-DD',\n",
        "          'end_date': 'YYYY-MM-DD',\n",
        "          'dataset_table': 'dataset_name.nl_lines_data_raw',\n",
        "          'additional_filter': 'AND column_name = \"value\"'  # Optional filter\n",
        "      }\n",
        "  )\n",
        "  # Task 3:\n",
        "  insert_to_bq_task = PythonOperator(\n",
        "    task_id='insert_to_bq',\n",
        "    python_callable=insert_to_bq,\n",
        "    op_kwargs={\n",
        "        'gsi_filepath': gs_filename,\n",
        "        'dataset_id': 'dataset_name',\n",
        "        'table_id': 'nl_lines_data_raw'\n",
        "    })\n",
        "\n",
        "  # Define task dependencies\n",
        "  gs_filename = get_data()\n",
        "  gs_filename >> delete_reloading_task >> insert_to_bq_task"
      ],
      "metadata": {
        "id": "ZUkwn8LENKhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}